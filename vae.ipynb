{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOI2ClaOX73tIAKRqUYGnB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yagmurakarken/Sprites/blob/master/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the code is taken from COMP447 HW question. The code is adapted for Sprites dataset."
      ],
      "metadata": {
        "id": "KZ8JNeXO6xY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcom1n876sMg"
      },
      "outputs": [],
      "source": [
        "!if [ -d deepul ]; then rm -Rf deepul; fi\n",
        "!git clone https://github.com/johnberg1/deepul.git\n",
        "!pip install ./deepul"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from deepul.hw2_helper import *"
      ],
      "metadata": {
        "id": "iyCEO3237AcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepul.pytorch_util as ptu\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import trange, tqdm_notebook\n",
        "import copy\n",
        "from torch.distributions.uniform import Uniform\n",
        "from torch.distributions.normal import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm"
      ],
      "metadata": {
        "id": "jh69Ytvd7Amq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "wtRnijxq7AtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch, quiet, grad_clip=None, prior=False):\n",
        "    model.train()\n",
        "\n",
        "    if not quiet:\n",
        "        pbar = tqdm(total=len(train_loader.dataset))\n",
        "    losses = OrderedDict()\n",
        "    for x in train_loader:\n",
        "        x = x.cuda()\n",
        "        if not prior:\n",
        "          x = x.float()\n",
        "        out = model.loss(x)\n",
        "        optimizer.zero_grad()\n",
        "        out['loss'].backward()\n",
        "        if grad_clip:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        desc = f'Epoch {epoch}'\n",
        "        for k, v in out.items():\n",
        "            if k not in losses:\n",
        "                losses[k] = []\n",
        "            losses[k].append(v.item())\n",
        "            avg_loss = np.mean(losses[k][-50:])\n",
        "            desc += f', {k} {avg_loss:.4f}'\n",
        "\n",
        "        if not quiet:\n",
        "            pbar.set_description(desc)\n",
        "            pbar.update(x.shape[0])\n",
        "    if not quiet:\n",
        "        pbar.close()\n",
        "    return losses\n",
        "\n",
        "\n",
        "def eval_loss(model, data_loader, quiet, prior):\n",
        "    model.eval()\n",
        "    total_losses = OrderedDict()\n",
        "    with torch.no_grad():\n",
        "        for x in data_loader:\n",
        "            x = x.cuda()\n",
        "            if not prior:\n",
        "              x = x.float()\n",
        "            out = model.loss(x)\n",
        "            for k, v in out.items():\n",
        "                total_losses[k] = total_losses.get(k, 0) + v.item() * x.shape[0]\n",
        "\n",
        "        desc = 'Test '\n",
        "        for k in total_losses.keys():\n",
        "            total_losses[k] /= len(data_loader.dataset)\n",
        "            desc += f', {k} {total_losses[k]:.4f}'\n",
        "        if not quiet:\n",
        "            print(desc)\n",
        "    return total_losses\n",
        "\n",
        "\n",
        "def train_epochs(model, train_loader, test_loader, train_args, quiet=False, prior=False):\n",
        "    epochs, lr = train_args['epochs'], train_args['lr']\n",
        "    grad_clip = train_args.get('grad_clip', None)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, test_losses = OrderedDict(), OrderedDict()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = train(model, train_loader, optimizer, epoch, quiet, grad_clip, prior=prior)\n",
        "        test_loss = eval_loss(model, test_loader, quiet, prior=prior)\n",
        "\n",
        "        for k in train_loss.keys():\n",
        "            if k not in train_losses:\n",
        "                train_losses[k] = []\n",
        "                test_losses[k] = []\n",
        "            train_losses[k].extend(train_loss[k])\n",
        "            test_losses[k].append(test_loss[k])\n",
        "    return train_losses, test_losses"
      ],
      "metadata": {
        "id": "NqDYB9G57AwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sprites.load_sprites import sprites_act\n",
        "X_train, X_test, A_train, A_test, D_train, D_test = sprites_act('Sprites/', return_labels=True)"
      ],
      "metadata": {
        "id": "ZbiWV8lg7Ayt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_new=np.concatenate((X_train[0], X_train[1],X_train[2],X_train[3],X_train[4]), axis=0)\n",
        "test_new=np.concatenate((X_test[0], X_test[1],X_test[2],X_test[3],X_test[4]), axis=0) \n",
        "print(train_new.shape)\n",
        "print(np.concatenate((X_train[0], X_train[1], X_train[2]), axis=0).shape)\n",
        "\n",
        "for i in range(1000):\n",
        "  train_new=np.concatenate((train_new, X_train[i+5]), axis=0)\n",
        "print(train_new.shape)\n",
        "for i in range(100):\n",
        "  test_new=np.concatenate((test_new, X_test[i+5]), axis=0)"
      ],
      "metadata": {
        "id": "PVdGN-W57A1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_new = train_new.astype(np.float32)\n",
        "train_new = 255 * train_new\n",
        "train_new = train_new.astype(np.uint8)\n",
        "print(type(train_new[0][0][0][0]))\n",
        "\n",
        "test_new = test_new.astype(np.float32)\n",
        "test_new = 255 * test_new\n",
        "test_new = test_new.astype(np.uint8)\n",
        "print(type(test_new[0][0][0][0]))"
      ],
      "metadata": {
        "id": "yivcSqxU7llY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_image = random.randint(0, len(train_new))\n",
        "plt.imshow(train_new[random_image])\n",
        "plt.title(f\"Training example #{random_image}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y8Vn50W_8Dhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvEncoder(nn.Module):\n",
        "  ###################### PSEUDOCODE ##########################\n",
        "  # Encoder\n",
        "  #   conv2d(3, 32, 3, 1, 1)\n",
        "  #   relu()\n",
        "  #   conv2d(32, 64, 3, 2, 1) # 16 x 16\n",
        "  #   relu() \n",
        "  #   conv2d(64, 128, 3, 2, 1) # 8 x 8\n",
        "  #   relu()\n",
        "  #   conv2d(128, 256, 3, 2, 1) # 4 x 4\n",
        "  #   relu()\n",
        "  #   flatten()\n",
        "  #   linear(4 * 4 * 256, 2 * latent_dim)\n",
        "\n",
        "    def __init__(self,latent_size):\n",
        "        super().__init__()\n",
        "        self.latent_dim=latent_size\n",
        "        self.convs = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 4, 2, 1),          # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 32, 4, 2, 1),          # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, 4, 2, 1),          # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 4, 2, 1),          # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64, 256, 4, 1),            # B, 256,  1,  1\n",
        "            nn.ReLU(True),\n",
        "            \n",
        "        )\n",
        "            \n",
        "        self.fc=nn.Linear(256, latent_size*2)            # B, z_dim*2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.convs(x)\n",
        "        out=out.view(-1, 256*1*1)\n",
        "        mu, log_std = self.fc(out).chunk(2, dim=1)\n",
        "        return mu, log_std"
      ],
      "metadata": {
        "id": "8YIsYRsL7lq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvDecoder(nn.Module):\n",
        "  ###################### PSEUDOCODE ##########################\n",
        "  # Decoder\n",
        "  #   linear(latent_dim, 4 * 4 * 128)\n",
        "  #   relu()\n",
        "  #   reshape(4, 4, 128)\n",
        "  #   transpose_conv2d(128, 128, 4, 2, 1) # 8 x 8\n",
        "  #   relu()\n",
        "  #   transpose_conv2d(128, 64, 4, 2, 1) # 16 x 16\n",
        "  #   relu()\n",
        "  #   transpose_conv2d(64, 32, 4, 2, 1) # 32 x 32\n",
        "  #   relu()\n",
        "  #   conv2d(32, 3, 3, 1, 1)\n",
        "\n",
        "    def __init__(self, latent_size):\n",
        "        super().__init__()\n",
        "        self.latent_dim=latent_size\n",
        "        self.fc = nn.Linear(latent_size, 256)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 64, 4),      # B,  64,  4,  4\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 64, 4, 2, 1), # B,  64,  8,  8\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1), # B,  32, 16, 16\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 32, 4, 2, 1), # B,  32, 32, 32\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1),  # B, nc, 64, 64\n",
        "        )\n",
        "         \n",
        "        \n",
        "\n",
        "    def forward(self, z):\n",
        "        batch=z.shape[0]\n",
        "        out = self.fc(z)\n",
        "        out = out.view(-1, 256, 1, 1)\n",
        "        out=self.decoder(out)\n",
        "\n",
        "\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "2nQyJQE-7lwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaVAE(nn.Module):\n",
        "\n",
        "  # Use ConvEncoder and ConvDecoder in init function\n",
        "  # encoder = ConvEncoder\n",
        "  # decoder = ConvDecoder\n",
        "  def __init__(self,latent_size):\n",
        "      super().__init__()\n",
        "      self.latent_size=latent_size\n",
        "      self.encoder=ConvEncoder(latent_size)\n",
        "      self.decoder=ConvDecoder(latent_size)\n",
        "\n",
        "  # implement loss function (ELBO)\n",
        "  def loss(self,x):\n",
        "\n",
        "    mu, log_std = self.encoder(x)\n",
        "    z =  mu+torch.randn_like(mu) * log_std.exp() \n",
        "    x_recon = self.decoder(z)\n",
        "    \n",
        "  \n",
        "    recon_loss = torch.mean(torch.sum(F.mse_loss(x, x_recon, reduction='none').view(x.shape[0], -1), dim=1).float())\n",
        "    kl_loss =(torch.exp(2 * log_std) + mu ** 2) * 0.5  -log_std - 0.5\n",
        "    kl_loss = torch.mean(torch.sum(kl_loss, dim=1).float())\n",
        "    \n",
        "\n",
        "    return OrderedDict(loss=recon_loss + kl_loss, recon_loss=recon_loss,\n",
        "                           kl_loss=kl_loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # sampling function is already provided to you, you may wish to modify it if your model implementation is not compatible\n",
        "  def sample(self, n):\n",
        "      with torch.no_grad():\n",
        "          z = torch.randn(n, self.latent_size).cuda()\n",
        "          samples = torch.clamp(self.decoder(z), -1, 1)\n",
        "      return samples.cpu().permute(0, 2, 3, 1).numpy()"
      ],
      "metadata": {
        "id": "40Jt5HDg7lz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q2_a(train_data, test_data):\n",
        "    \"\"\"\n",
        "    train_data: A (n_train, H, W, 3) uint8 numpy array of quantized images with values in {0, 1, 2, 3}\n",
        "    test_data: A (n_test, H, W, 3) uint8 numpy array of binary images with values in {0, 1, 2, 3}\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, 3) numpy array of full negative ELBO, reconstruction loss E[-log p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated every minibatch\n",
        "    - a (# of epochs + 1, 3) numpy array of full negative ELBO, reconstruciton loss E[-p(x|z)],\n",
        "      and KL term E[KL(q(z|x) | p(z))] evaluated once at initialization and after each epoch\n",
        "    - a (100, 32, 32, 3) numpy array of 100 samples from your VAE with values in {0, ..., 255}\n",
        "    - a (100, 32, 32, 3) numpy array of 50 real image / reconstruction pairs\n",
        "      FROM THE TEST SET with values in {0, ..., 255}, code is already provided for this\n",
        "    - a (50, 32, 32, 3) numpy array of interpolations, code is already provided for this\n",
        "    \"\"\"\n",
        "\n",
        "    model = VanillaVAE(32).cuda()\n",
        "    train_data = (np.transpose(train_new, (0, 3, 1, 2)) / 255.).astype('float32')\n",
        "    test_data = (np.transpose(test_new, (0, 3, 1, 2)) / 255.).astype('float32')\n",
        "    train_loader = data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "    test_loader = data.DataLoader(test_data, batch_size=128)\n",
        "\n",
        "    train_losses, test_losses = train_epochs(model, train_loader, test_loader,\n",
        "                                             dict(epochs=20, lr=1e-3), quiet=True)\n",
        "    train_losses = np.stack((train_losses['loss'], train_losses['recon_loss'], train_losses['kl_loss']), axis=1)\n",
        "    test_losses = np.stack((test_losses['loss'], test_losses['recon_loss'], test_losses['kl_loss']), axis=1)\n",
        "    samples = model.sample(100) * 255.\n",
        "\n",
        "    # The following lines are for creating reconstructions and interpolations from the test set\n",
        "    # They should be working if you created your encoder, decoder correctly and named them encoder, decoder\n",
        "    # Feel free to modify them if your model is not compatible with this piece of code\n",
        "    x = next(iter(test_loader))[:50].cuda().float()\n",
        "    with torch.no_grad():\n",
        "        #x = rescale(x)\n",
        "        z, _ = model.encoder(x)\n",
        "        x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
        "    reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 64, 64)\n",
        "    reconstructions = reconstructions.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
        "\n",
        "    x = next(iter(test_loader))[11:12].float().cuda()\n",
        "    good_indices = [29,30,31]\n",
        "    with torch.no_grad():\n",
        "        #x = rescale(x)\n",
        "        z, _ = model.encoder(x)\n",
        "        z1 = torch.clone(z)\n",
        "        out_interps = []\n",
        "        for dim in good_indices:\n",
        "          all_interps = []\n",
        "          for alpha in np.linspace(0, 5, 10):\n",
        "              z3 = torch.clone(z1)\n",
        "              z3[:,dim] = z1[:,dim] + alpha\n",
        "              interp = model.decoder(z3)\n",
        "              all_interps.append(interp)\n",
        "          all_interps = torch.stack(all_interps, dim=1).view(-1, 3, 64, 64)\n",
        "          out_interps.append(all_interps)\n",
        "    out_interps = torch.stack(out_interps, dim=0)\n",
        "    out_interps = out_interps.view(-1, 3, 64, 64)\n",
        "    out_interps = out_interps\n",
        "    interps = out_interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
        "\n",
        "    return train_losses, test_losses, samples, reconstructions, interps"
      ],
      "metadata": {
        "id": "f6yXbosL7l31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_save_results('a', q2_a)"
      ],
      "metadata": {
        "id": "gBnXkDSX7l7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38mgUGOO7mAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mqasp2zi7mFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhAf_Z5d7mK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}